{
  "hash": "059b5582ffd4d80949db62dc84c5fe26",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Text Preprocessing for NLP\" # Blog post title\ndescription: \"Whaaat? You're telling me math can read??\" # Blog description\ndate: 07-24-2024 # Date this post is published\ncategories: [Deep Learning, NLP, Python] # Self-defined categories separated by commas\n---\n\n\n# Introduction\n\nNatural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. It is used for things like sentiment analysis, product reviews, text summary, text generation, language translation, and countless more. Essentially, if you want to solve an issue that involves natural language, it's an NLP problem.\n\nThe early days of NLP involved a bunch of scientists creating if-else statements to support a large set of rules, so it was very limited on what you can accomplish. Since then, AI has come a long way and we can now use neural networks, like a recurrent neural network (RNN), to work with text data. Luckily for us, frameworks like `TensorFlow` and `PyTorch` allow us to easily create these types of models where they can learn from text data and produce whatever we're looking for. Most people use python for deep learning, so any code mentioned will be python. Some popular libraries for NLP in python are `NLTK` (Natural Language Tool Kit) which is used for cleaning text data and applying some attributes mentioned later, and `TensorFlow` for the actual model creation. `TensorFlow` in this post's case is used for tokenization, and some other languages get used for smaller parts like `spacy`, `re`, `spellchecker`, and `collections.`\n\nWhen you want to create a neural network to run an NLP task, like a Long Short-Term Memory (LSTM) recurrent neural network, you have to clean the text for the computer to be able to find patterns in the first place. When using something like a transformer such as GPT-4 or BERT, most of the text preprocessing is done in the model so it won't be necessary to clean yourself, but the techniques mentioned bellow assume you want to know how to preprocess text data for non-transformer based models. Text preprocessing usually involves cleaning, tokenizing (breaking sentences into words), and embedding (representing words as vectors where similar words are closer together numerically).\n\n# Text Cleaning\n\nText cleaning is the process of preparing your text before converting words into numerical representations for your model to find patterns. The primary goal is to ensure that different forms of the same word are treated as equal. For example, \"long,\" \"Long,\" and \"Looong\" should all be recognized as \"long\" by the computer. This section outlines the fundamental steps to achieve a normalized dataset, which you can adjust based on your specific needs.\n\n## Lowercasing\n\nConvert all text to lowercase to avoid treating something like \"Hot\" and \"hot\" as different words.\n\nPython Example:\n\n::: {#e5414fa0 .cell execution_count=1}\n``` {.python .cell-code}\ndef lowercase(text):\n  return text.lower()\n```\n:::\n\n\n## URLs\n\nRemove URLs to retain only the text.\n\nPython Example:\n\n::: {#5148e07d .cell execution_count=2}\n``` {.python .cell-code}\nimport re\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n```\n:::\n\n\n## Emoticons\n\nRemove emoticons and emojis, as they do not convey meaning to the computer in the same way words do\n\nPython Example:\n\n::: {#edca9206 .cell execution_count=3}\n``` {.python .cell-code}\nimport re\n\ndef remove_emoticons(text):\n  emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n  return emoji_pattern.sub(r'', text)\n```\n:::\n\n\n## Special Characters & Numbers\n\nRemove special characters and numbers, unless they are relevant to your analysis.\n\nPython Example:\n\n::: {#32db615e .cell execution_count=4}\n``` {.python .cell-code}\nimport re\n\ndef remove_characters(text):\n  return re.sub(r'[^A-Za-z ]+', '', text)\n```\n:::\n\n\n## Stop Words\n\nRemove stop words, which are common words that do not contribute much to the meaning.\n\nPython Example:\n\n::: {#14bd2803 .cell execution_count=5}\n``` {.python .cell-code}\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words(\"english\"))\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)\n```\n:::\n\n\n## Spell Correction\n\nCorrect spelling mistakes to ensure consistency in word representation.\n\nPython Example:\n\n::: {#d63cd6c7 .cell execution_count=6}\n``` {.python .cell-code}\nfrom spellchecker import SpellChecker\n\ndef correct_spellings(text):\n    if not text.strip():\n        return text\n\n    spell = SpellChecker()\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n\n    for word in text.split():\n        if word in misspelled_words:\n            suggested_correction = spell.correction(word)\n            if suggested_correction and \"'\" not in suggested_correction:\n                corrected_text.append(suggested_correction)\n            else:\n                corrected_text.append(word)\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n```\n:::\n\n\n## Part-of-Speech\n\nTag each word with its part of speech to provide context to the model.\n\nPython Example:\n\n::: {#edf91d33 .cell execution_count=7}\n``` {.python .cell-code}\nimport nltk\nnltk.download('averaged_perceptron_tagger')\n\ndef get_wordnet_pos(text):\n    tag = nltk.pos_tag([text])[0][1][0].upper()\n    tag_dict = {\"J\": \"a\",  # Adjective\n                \"N\": \"n\",  # Noun\n                \"V\": \"v\",  # Verb\n                \"R\": \"r\"}  # Adverb\n    return tag_dict.get(tag, \"n\")\n```\n:::\n\n\n## Lemmatizing vs Stemming\n\nOnce you've made it this far, should you lemmatize or stem your words? Lemmatization is when you cut the suffix off a word and replace it with the normalized form of the word, while stemming refers to cutting the suffix off completely. A good example of this is if you have the word \"running\", the stemmed version would be \"runn\" while the lemmatized version would be \"run\". You can see that by lemmatizing your words, you convert it into it's root word, while stemming could create almost like a second version of the word, which your model might see as a completely separate word. This is why I highly suggest **lemmatizing** your words rather than stemming them.\n\nPython Example:\n\n::: {#74a312ce .cell execution_count=8}\n``` {.python .cell-code}\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('punkt')\nnltk.download('wordnet')\n\ndef lemmatize(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(text)]\n    return \" \".join(lemmatized_words)\n```\n:::\n\n\n## Other Options\n\nThere is more out there that you can do to clean or preprocess your text data. Some examples could be marking negations, handling specific elements like hashtags, converting dates to the same format, converting numbers to words, etc. Some of the cleaning mentioned before you could possibly even avoid, depending on what your end goal is. If you're planning on using a transformer in the future, pretty much all this text preprocessing is done fully for you through pre-trained models which would cut a lot of time from this stage.\n\n# Tokenization\n\nAfter cleaning, split sentences into words (tokens), create sequences, and pad them to ensure consistent vector lengths.\n\n::: {#f7d63c4b .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\n# Define parameters\nMAX_VOCAB_SIZE = max_vocab_size  # Maximum number of words to keep in the vocabulary\nMAX_SEQUENCE_LENGTH = max_seq_length  # Maximum length of each sequence\n\n# Assuming you have these variables defined:\n# all_texts: list of all text samples\n# train_texts, val_texts, test_texts: lists of text samples for each set\n# train_labels, val_labels, test_labels: corresponding labels\n\n# Tokenization\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(all_texts)  # Fit tokenizer on all available texts\n\n# Convert texts to sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_texts)\nval_sequences = tokenizer.texts_to_sequences(val_texts)\ntest_sequences = tokenizer.texts_to_sequences(test_texts)\n\n# Pad sequences\ntrain_padded = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\nval_padded = pad_sequences(val_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\ntest_padded = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n\n# Convert labels to numpy arrays\ntrain_labels = np.array(train_labels)\nval_labels = np.array(val_labels)\ntest_labels = np.array(test_labels)\n\n# Now train_padded, val_padded, and test_padded are your vectorized inputs\n# train_labels, val_labels, and test_labels are your corresponding labels\n```\n:::\n\n\n# Pre-trained Embeddings\n\nThe final part of text preprocessing are embeddings. Realistically, unless you have a crazy compute power, you're not going to be training your own embeddings, so you can use something like [GloVe](https://nlp.stanford.edu/projects/glove/), [Word2Vec](https://arxiv.org/abs/1301.3781), or others like these where people have embedded lots of words to find their spatial relationships to each other. Below I have an example of GloVe, and it would create the vector representations of your words you've just cleaned, and tokenized, and make it ready to put into your deep learning model.\n\nIf in the future you are planning on using a pre-trained model like BERT or GPT, then you would not need to do this step as the model will already do all the work for you.\n\n::: {#ecdb5b96 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\n\ndef load_glove_embeddings(filepath, word_index, embedding_dim):\n    \"\"\"\n    Load GloVe embeddings for the words in the tokenizer's word index.\n    \n    Args:\n    filepath (str): Path to the GloVe embeddings file.\n    word_index (dict): Word-to-index mapping from the tokenizer.\n    embedding_dim (int): Dimensionality of the GloVe embeddings.\n    \n    Returns:\n    numpy.ndarray: Embedding matrix for the vocabulary.\n    \"\"\"\n    vocab_size = len(word_index) + 1  # Adding 1 for padding token\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n    \n    with open(filepath, encoding=\"utf8\") as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            if word in word_index:\n                idx = word_index[word]\n                embedding_matrix[idx] = np.asarray(values[1:], dtype='float32')\n    \n    return embedding_matrix\n\n# Example usage:\n# Assuming you have already created and fitted a tokenizer\nGLOVE_PATH = 'path/to/glove.42B.300d.txt'  # Update this path\nEMBEDDING_DIM = 300\n\n# Load GloVe embeddings\nembedding_matrix = load_glove_embeddings(GLOVE_PATH, tokenizer.word_index, EMBEDDING_DIM)\n\n# Now you can use this embedding_matrix in your model, for example:\n# model.add(Embedding(vocab_size, EMBEDDING_DIM, \n#                     weights=[embedding_matrix], \n#                     input_length=MAX_SEQUENCE_LENGTH, \n#                     trainable=False))\n```\n:::\n\n\n# Conclusion\n\nAt this point you should have cleaned your text data, tokenized it, created padded sequences for the tokens, and then used pre-trained word embeddings to vectorize and find relationships within the data. Now you can feed it into a neural network! I would suggest using a Bidirectional Long Short-Term Memory RNN to train your model as this seems to be a good, safe option when training a model for NLP yourself.\n\nThere's a lot to be done when trying to process text for NLP, and now you know how to turn natural language into computer language! One that would then have to be turned from numbers into the actual computer's language... Anyways, go train a custom model! Natural language processing is everywhere in the world now, and has great applications in literally everything. If you have ever made a stock trading bot where you use machine learning to analyze trends within numeric data to create predictions, you can actually use NLP to review articles of companies and create sentiment analysis to determine if the stock will change. Nevertheless, remember to preprocess all text until it's as bare as it can get, and your model will be happy!\n\nSide note, I believe that because of the creation of transformers, most recurrent neural networks (yes, even the better optimized ones) aren't nearly as good as transformers when it comes to pattern recognition and memorization of text. The transformer models that are currently out for consumer use like Llama 3, GPT-4o, Claude 3, etc. are all trained with billions of dollars so they work very, VERY well. You know who doesn't have billions of dollars? Probably you (I'm sorry), so generally using a pre-trained transformer model is the way to go if you want to do any projects involving NLP. You can even fine-tune one of these models so it can perform better at a specific task. I think it's good to know the different core neural networks before tackling something like a transformer, but transformers are kind of the way to go now. You can thank NVIDIA for that.\n\n------------------------------------------------------------------------\n\n# Support <i class=\"fa-solid fa-heart\"></i> {.unlisted}\n\nYou've reached the end of my blog post! 🎉🎉🎉\\\nIf you learned something and would like to support me, feel free to buy me a coffee :)\n\n\n```{=html}\n<script type=\"text/javascript\" src=\"https://cdnjs.buymeacoffee.com/1.0.0/button.prod.min.js\" data-name=\"bmc-button\" data-slug=\"coreymiichaud\" data-color=\"#FFDD00\" data-emoji=\"\"  data-font=\"Poppins\" data-text=\"Buy me a coffee\" data-outline-color=\"#000000\" data-font-color=\"#000000\" data-coffee-color=\"#ffffff\" ></script>\n```\n\n# Comments {.unlisted}\n\n\n```{=html}\n<script src=\"https://utteranc.es/client.js\"\n        repo=\"coreymichaud/coreymichaud.github.io\"\n        issue-term=\"url\"\n        theme=\"github-light\"\n        crossorigin=\"anonymous\"\n        async>\n</script>\n```\n\n",
    "supporting": [
      "index_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}