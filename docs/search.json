[
  {
    "objectID": "posts/16/index.html",
    "href": "posts/16/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "posts/14/index.html",
    "href": "posts/14/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "posts/12/index.html",
    "href": "posts/12/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Corey Michaud",
    "section": "",
    "text": "Hey!\nMy name’s Corey Michaud. I recently graduated from the University of Central Florida with a B.S. in Statistics. I have a passion for using statistics, with the addition of coding, to solve real-world problems and create visualizations to better represent data.\n\n\nEducation\nUniversity of Central Florida | Orlando, Florida\nB.S. in Statistics | Aug. 2020 - May 2024\n\n\nSkills\nProgramming/Scripting Languages: Python, R, SQL, HTML5, CSS3\nData Visualization: Tableau, Power BI, ggplot2, seaborn\nTools: Excel, Git, SPSS"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Education\nI graduated from the University of Central Florida in the Spring of 2024 with a B.S. in Statistics. I majored in Statistics because throughout high school I was pretty good at math, and when I took AP Statistics my junior year I felt like it came naturally to me. I “decided” on a new major every few months in High School until I landed on statistics, and I’m very glad I chose it. At UCF I was able to participate in the 2023 Big Data Symposium where I got the opportunity to learn about different jobs that used statistics and machine learning. I mingled with a lot of professors where I heard about their research and found out what it was like doing statistics in academia. I believe I got to do a lot of fun experiences in college just by simply majoring in statistics, which is very cool!\n\nDuring my time at UCF, I was a part of three clubs: The Association of Computing Machine, Knight Hacks, and AI@UCF. Through these clubs I was able to hone my coding skills and learn about different subjects within machine learning like reinforcement learning and deep learning. These clubs allowed me to attend seminars on different fields within math computer science, and I got hands-on experience with a number of data structures and algorithms. Knight Hacks, for instance, holds one of the largest Florida Hackathons each year, and I was able to attend it every year where I had to make small projects under a time constraint, constant pressure, and a lack of sleep.\nCourses that I have taken include statistical methods, statistical theory, nonparametric statistics, calculus, linear algebra, machine learning, and time series. These have all helped me grow my knowledge of statistics and machine learning and apply it to case studies and numerous projects. One thing that I was shocked about was that statistics, a math degree, required a lot of presentations, so I was able to obtain a lot of experience with explaining data and results in both technical and non-technical ways. I have also covered other areas like Bayesian statistics and mathematical methods for artificial intelligence, but these have been spread out through some of my classes.\n  \n\n\n\n Skills\nCoding wise, I mainly have used Python and R for machine learning, and R I have used the most for also conducting statistical tests. With R, I am able to create visualizations using ggplot, manipulate and clean data, and using different libraries I can do many machine learning functions to solve problems within data. For Python, I have used matplotlib and seaborn for data visualization, and I have used sci-kit learn for shallow machine learning. I have used Keras and TensorFlow for a natural language processing project, but I want to learn more about both languages, and hopefully PyTorch soon. When I need to communicate with databases, I use SQL to write and have used SQLite and PostgreSQL for database administration.\nData visualization has always interested me since I made my first “pretty” plot using ggplot in R. Since then, I have gotten very good at powerfull and aesthetic visualizations in R and Python, but now I am focusing a lot on Tableau and Power BI for their interactive dashboard capability.\nFeel free to check out my projects tab where I showcase some of them! If you want to check out more, or the source code, then be sure to click on the GitHub icon in the navigation bar.\n\n\n\n\n Hobbies\nWhen I’m not trying to learn new things, I like to play video games. Minecraft is an oldie but a goodie, and I sometimes like to play Hearthstone because you can multitask while playing the battlegrounds mode. I also love watching movies, even ones people say are “bad”, but I especially like watching them in a movie theater. I feel like it’s a lot more immersive and fun that way.\n\nI’m not sure if this counts as a hobby, but exploring new areas excites me. Everything is new and interesting, and you get to feel what it’s like to live in that area. When I still lived with my family, we would travel a lot, and I got to experience national parks, loud cities, and nature that I don’t get to experience a lot here in Florida. One thing I’ve learned recently is Florida has a lot of natural springs and, to me, that feels like traveling because they’re far from where I live, and they all feel different from each other. Yes, they all have water to swim and kayak in, but the surrounding areas are unique in their own ways.\n  \n\n\n\n Cats\n \nMy girlfriend, Kristina, and I have 2 cats. A cranky, 7 year old tabby named Gnocchi (left), and a super hungry 2 year old calico named Ube (right). Gnocchi is a difficult cat who likes to decide that she doesn’t like her fancy wet food anymore, and request different food. She also happens to be a yapper if you don’t give her enough attention, so she gets played with a lot. Ube, on the other hand, is a purr-fect cat. She plays with toys and whatever random wrappers she can find, and likes to cuddle. The one thing about Ube is she can eat, eat, eat. Take her to a buffet and she could probably clean the whole place out."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Statosphere",
    "section": "",
    "text": "Welcome to my blog, The Statosphere. Currently, this is a place where I can write while I do a lot of self-study on, but not limited to, statistics, machine learning, and various fields of mathematics. Hopefully by writing in a bloggy way, I can help myself learn while teaching you something new or interesting. So put your reading glasses on and get ready to enter The Statosphere.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\nJul 24, 2024\n\n\nText Preprocessing for NLP\n\n\nCorey Michaud\n\n\n12 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/06-25-2024-Test-Post/index.html",
    "href": "posts/06-25-2024-Test-Post/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "posts/13/index.html",
    "href": "posts/13/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "posts/15/index.html",
    "href": "posts/15/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "posts/Blog Template/index.html",
    "href": "posts/Blog Template/index.html",
    "title": "Insert title",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud,\n  author = {Michaud, Corey},\n  title = {Insert Title},\n  url = {https://coreymichaud.github.io/nameofthisblogsfolder/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. n.d. “Insert Title.” https://coreymichaud.github.io/nameofthisblogsfolder/."
  },
  {
    "objectID": "blog/16/index.html",
    "href": "blog/16/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "blog/14/index.html",
    "href": "blog/14/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "blog/12/index.html",
    "href": "blog/12/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "blog/06-25-2024-Test-Post/index.html",
    "href": "blog/06-25-2024-Test-Post/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "blog/13/index.html",
    "href": "blog/13/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "blog/15/index.html",
    "href": "blog/15/index.html",
    "title": "Test Post",
    "section": "",
    "text": "Hello to my blog post!\n\n\n\nCitationBibTeX citation:@online{michaud2024,\n  author = {Michaud, Corey},\n  title = {Test {Post}},\n  date = {2024-06-25},\n  url = {https://github.com/coreymichaud},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nMichaud, Corey. 2024. “Test Post.” June 25, 2024. https://github.com/coreymichaud."
  },
  {
    "objectID": "blog/06-27-2024-Industrial-Revolution/index.html",
    "href": "blog/06-27-2024-Industrial-Revolution/index.html",
    "title": "The American Industrial Revolution",
    "section": "",
    "text": "The American Industrial Revolution, spanning roughly from the late 18th century to the mid-19th century, was a period of profound economic, social, and technological transformation. This era marked the transition from agrarian economies to industrialized and urbanized societies. Several key factors contributed to this transformation, each playing a pivotal role in reshaping the United States."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Los Angeles Crime Visualized\n\n\n\n\n\n\nR\n\n\nVisualization\n\n\n\nVisualization of crime in LA with R and ggplot2.\n\n\n\n\n\nJul 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nNeural Network From Scratch\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\n\nClassification of MNIST handwritten digits from scratch in Python.\n\n\n\n\n\nJun 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAlzheimer’s Detection\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\n\nThis is a python notebook where I analyze the DAWRIN dataset to detect Alzheimer’s in an individual’s handwriting. To classify, I used random forest in Python.\n\n\n\n\n\nJul 31, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Alzheimer’s Detection",
    "section": "",
    "text": "Source Code\nHi"
  },
  {
    "objectID": "projects/alzheimers-detection/index.html",
    "href": "projects/alzheimers-detection/index.html",
    "title": "Alzheimer’s Detection",
    "section": "",
    "text": "Source Code\n\nBackground\nAlzheimer’s is a type of dementia that affects memory, thinking, and behavior. It is caused by increasing age, and primarily affects people above the age of 65. As a person develops Alzheimer’s, it progressively becomes worse where the individual can lose the ability to carry a conversation or even take care of themselves. After diagnosis, a person can expect to live on average between 4 to 8 years, but on better cases up to 20 years. Luckily there is medication to help slow the worsening of Alzheimer’s, but nothing to completely prevent it from happening.\nThe data used for the detection of Alzheimer’s through handwriting comes from the DARWIN (Diagnosis AlzheimeR WIth haNdwriting) dataset. This dataset is made up of 174 individual’s handwriting where roughly half are Alzheimer’s patients (P), and healthy people (H). The handwriting was taken through tasks the individuals were asked to do, and then variables like time in air were measured. In doing so, the creators of the DARWIN dataset provided us the materials we need to use machine learning techniques to detect the early stages of Alzheimer’s through handwriting. Some of the tasks recorded were connecting points through lines and copying phrases that were written in front of them, all of which test different parts of the brain.\nUsing handwriting data, I will use a random forest classifier to predict whether an individual has Alzheimer’s or not. The goal is for future handwriting data to be inserted and accurately predict the correct diagnosis, saving the individual time to get treatment to slow down the process.\nAlzheimers detection dataset obtained from https://www.kaggle.com/datasets/taeefnajib/handwriting-data-to-detect-alzheimers-disease.\n\n# Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Loading data\nalz = pd.read_csv(\"alzheimers.csv\")\n\n\n\nExploratory Data Analysis\n\n# First 5 rows of data\nalz.head(5)\n\n\n\n\n\n\n\n\nID\nair_time1\ndisp_index1\ngmrt_in_air1\ngmrt_on_paper1\nmax_x_extension1\nmax_y_extension1\nmean_acc_in_air1\nmean_acc_on_paper1\nmean_gmrt1\n...\nmean_jerk_in_air25\nmean_jerk_on_paper25\nmean_speed_in_air25\nmean_speed_on_paper25\nnum_of_pendown25\npaper_time25\npressure_mean25\npressure_var25\ntotal_time25\nclass\n\n\n\n\n0\nid_1\n5160\n0.000013\n120.804174\n86.853334\n957\n6601\n0.361800\n0.217459\n103.828754\n...\n0.141434\n0.024471\n5.596487\n3.184589\n71\n40120\n1749.278166\n296102.7676\n144605\nP\n\n\n1\nid_2\n51980\n0.000016\n115.318238\n83.448681\n1694\n6998\n0.272513\n0.144880\n99.383459\n...\n0.049663\n0.018368\n1.665973\n0.950249\n129\n126700\n1504.768272\n278744.2850\n298640\nP\n\n\n2\nid_3\n2600\n0.000010\n229.933997\n172.761858\n2333\n5802\n0.387020\n0.181342\n201.347928\n...\n0.178194\n0.017174\n4.000781\n2.392521\n74\n45480\n1431.443492\n144411.7055\n79025\nP\n\n\n3\nid_4\n2130\n0.000010\n369.403342\n183.193104\n1756\n8159\n0.556879\n0.164502\n276.298223\n...\n0.113905\n0.019860\n4.206746\n1.613522\n123\n67945\n1465.843329\n230184.7154\n181220\nP\n\n\n4\nid_5\n2310\n0.000007\n257.997131\n111.275889\n987\n4732\n0.266077\n0.145104\n184.636510\n...\n0.121782\n0.020872\n3.319036\n1.680629\n92\n37285\n1841.702561\n158290.0255\n72575\nP\n\n\n\n\n5 rows × 452 columns\n\n\n\n\n# Shape of data\nalz.shape\n\n(174, 452)\n\n\n\n# Data information\nalz.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 174 entries, 0 to 173\nColumns: 452 entries, ID to class\ndtypes: float64(300), int64(150), object(2)\nmemory usage: 614.6+ KB\n\n\n\n# Checking for object column names\nalz.select_dtypes(include = \"object\").columns.tolist()\n\n['ID', 'class']\n\n\n\n# Checking for missing values\nalz.isna().sum() # No NA values\n\nID                 0\nair_time1          0\ndisp_index1        0\ngmrt_in_air1       0\ngmrt_on_paper1     0\n                  ..\npaper_time25       0\npressure_mean25    0\npressure_var25     0\ntotal_time25       0\nclass              0\nLength: 452, dtype: int64\n\n\n\n\nFeature Engineering\n\n# Removing ID column\nalz = alz.drop(\"ID\", axis = 1)\nalz.head(5)\n\n\n\n\n\n\n\n\nair_time1\ndisp_index1\ngmrt_in_air1\ngmrt_on_paper1\nmax_x_extension1\nmax_y_extension1\nmean_acc_in_air1\nmean_acc_on_paper1\nmean_gmrt1\nmean_jerk_in_air1\n...\nmean_jerk_in_air25\nmean_jerk_on_paper25\nmean_speed_in_air25\nmean_speed_on_paper25\nnum_of_pendown25\npaper_time25\npressure_mean25\npressure_var25\ntotal_time25\nclass\n\n\n\n\n0\n5160\n0.000013\n120.804174\n86.853334\n957\n6601\n0.361800\n0.217459\n103.828754\n0.051836\n...\n0.141434\n0.024471\n5.596487\n3.184589\n71\n40120\n1749.278166\n296102.7676\n144605\nP\n\n\n1\n51980\n0.000016\n115.318238\n83.448681\n1694\n6998\n0.272513\n0.144880\n99.383459\n0.039827\n...\n0.049663\n0.018368\n1.665973\n0.950249\n129\n126700\n1504.768272\n278744.2850\n298640\nP\n\n\n2\n2600\n0.000010\n229.933997\n172.761858\n2333\n5802\n0.387020\n0.181342\n201.347928\n0.064220\n...\n0.178194\n0.017174\n4.000781\n2.392521\n74\n45480\n1431.443492\n144411.7055\n79025\nP\n\n\n3\n2130\n0.000010\n369.403342\n183.193104\n1756\n8159\n0.556879\n0.164502\n276.298223\n0.090408\n...\n0.113905\n0.019860\n4.206746\n1.613522\n123\n67945\n1465.843329\n230184.7154\n181220\nP\n\n\n4\n2310\n0.000007\n257.997131\n111.275889\n987\n4732\n0.266077\n0.145104\n184.636510\n0.037528\n...\n0.121782\n0.020872\n3.319036\n1.680629\n92\n37285\n1841.702561\n158290.0255\n72575\nP\n\n\n\n\n5 rows × 451 columns\n\n\n\n\n# Converting class to numeric\nalz[\"class\"] = alz[\"class\"].replace({'P': 1, 'H': 0})\nalz[\"class\"]\n\nC:\\Users\\cor3y\\AppData\\Local\\Temp\\ipykernel_6484\\2961317950.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  alz[\"class\"] = alz[\"class\"].replace({'P': 1, 'H': 0})\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n169    0\n170    0\n171    0\n172    0\n173    0\nName: class, Length: 174, dtype: int64\n\n\n\n\nModel Training\n\nfrom sklearn.model_selection import train_test_split\n\n# Separating features from target\nX = alz.drop(columns=[\"class\"])\ny = alz[\"class\"]\n\n# Training data with a 70/30 split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 42)\n\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import plot_tree\n\n# Creating random forest pipeline with scaled data\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(random_state = 42, max_samples = 0.6, min_samples_leaf = 2))\n])\n\n# Fitting pipeline\npipe.fit(X_train, y_train)\n\n# Predicting target values\ny_pred = pipe.predict(X_test)\n\n\n# Plotting first tree in the random forest\ntree_viz = pipe.named_steps['classifier'].estimators_[0]\n\nfig, ax = plt.subplots(figsize = (15, 10))\n\nplot_tree(tree_viz, feature_names = alz.columns.tolist(), class_names = [\"Patient\", \"Healthy\"], filled = True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plotting fiftieth tree in the random forest\ntree_viz = pipe.named_steps['classifier'].estimators_[49]\n\nfig, ax = plt.subplots(figsize = (15, 10))\n\nplot_tree(tree_viz, feature_names = alz.columns.tolist(), class_names = [\"Patient\", \"Healthy\"], filled = True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nResults\n\nfrom sklearn.metrics import f1_score\n\n# F1 score is high so this random forest model is a good predictor of the target\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n\nF1 Score: 0.9019607843137255\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n# False positive and true positive rates\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\n# AUC\nauc = roc_auc_score(y_test, y_pred)\n\n# Plotting ROC curve\nfig, ax = plt.subplots()\n\nax.plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = {:.2f})'.format(auc))\nax.plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('Receiver Operating Characteristic (ROC) Curve')\nax.legend(loc = \"lower right\")\n\nsns.despine()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n# Creating confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\n\n# Plotting confusion matrix \nfig, ax = plt.subplots()\n\nsns.heatmap(conf_matrix,\n            annot = True,\n            fmt = 'g',\n            xticklabels = ['Positive', 'Negative'],\n            yticklabels = ['Positive', 'Negative'],\n            cmap = [\"Red\", \"Green\", \"Red\", \"Green\"],\n            cbar = False,\n            annot_kws = {\"size\": 20},\n            ax = ax)\n\nax.set_title('Confusion Matrix', fontsize = 17)\nax.set_ylabel('Prediction', fontsize = 13)\nax.set_xlabel('Actual', fontsize = 13)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating TP/FP/TN/FN\nTP = conf_matrix[1, 1]\nFN = conf_matrix[1, 0]\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\n\n# Printing results of predictions\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprecision = (TP) / (TP + FP)\nsensitivity = TP / (TP + FN)\nspecificity = TN / (TN + FP)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Specificity:\", specificity)\n\nAccuracy: 0.9056603773584906\nPrecision: 0.8846153846153846\nSensitivity: 0.92\nSpecificity: 0.8928571428571429"
  },
  {
    "objectID": "blog/06-27-2024-Industrial-Revolution/index.html#key-drivers-and-innovations",
    "href": "blog/06-27-2024-Industrial-Revolution/index.html#key-drivers-and-innovations",
    "title": "The American Industrial Revolution",
    "section": "",
    "text": "The introduction of mechanized textile production was a significant factor. Samuel Slater, known as the “Father of the American Industrial Revolution,” brought British textile technology to America, establishing the first successful textile mill in Pawtucket, Rhode Island, in 1793. The development of steam engines by innovators like James Watt revolutionized transportation and manufacturing. Steam-powered locomotives and boats facilitated faster movement of goods and people, effectively shrinking the vast American landscape. Pioneered by Eli Whitney, the concept of interchangeable parts in manufacturing standardized production and significantly increased efficiency, laying the groundwork for mass production.\n\n\n\nThe construction of canals, such as the Erie Canal (completed in 1825), and the expansion of the railroad network connected regional markets and resources, lowering transportation costs and spurring economic growth. Improved roadways facilitated better movement of goods and people within the country, promoting regional trade and commerce.\n\n\n\nThe population boom, fueled by both natural increase and immigration, provided a steady supply of labor for factories and industrial enterprises. Cities grew around industrial hubs, with places like Lowell, Massachusetts, becoming models of industrial communities. Urbanization brought about new social dynamics, including the rise of a distinct working class.\n\n\n\nThe establishment of a robust banking system provided the necessary capital for industrial ventures. The availability of credit and investment opportunities spurred entrepreneurial activities and industrial expansion. Protective tariffs and supportive government policies helped nurture American industries by shielding them from foreign competition and encouraging domestic production."
  },
  {
    "objectID": "blog/06-27-2024-Industrial-Revolution/index.html#economic-and-social-impact",
    "href": "blog/06-27-2024-Industrial-Revolution/index.html#economic-and-social-impact",
    "title": "The American Industrial Revolution",
    "section": "Economic and Social Impact",
    "text": "Economic and Social Impact\n\nEconomic Growth\nThe American Industrial Revolution led to unprecedented economic growth. The shift from agrarian to industrial economies increased productivity, diversified the economy, and boosted the national GDP. The rise of factories created numerous jobs, attracting rural populations to urban centers and fostering a consumer culture driven by new goods and services.\n\n\nSocial Changes\nThe harsh working conditions in factories gave rise to labor movements advocating for better wages, working hours, and conditions. This period saw the emergence of trade unions and labor strikes. Women and children played significant roles in the workforce, particularly in textile mills. This involvement began shifting traditional gender roles and highlighted the need for labor reforms.\n\n\nEnvironmental Impact\nIndustrialization brought significant environmental changes, including deforestation, pollution, and changes in land use. The exploitation of natural resources raised concerns about sustainability and environmental degradation.\n\n\nTechnological and Scientific Advancements\nThe industrial revolution spurred technological innovation and scientific discovery. Advances in metallurgy, chemistry, and engineering had far-reaching effects beyond industry, influencing agriculture, medicine, and everyday life."
  },
  {
    "objectID": "blog/06-27-2024-Industrial-Revolution/index.html#legacy",
    "href": "blog/06-27-2024-Industrial-Revolution/index.html#legacy",
    "title": "The American Industrial Revolution",
    "section": "Legacy",
    "text": "Legacy\nThe American Industrial Revolution laid the foundation for the United States’ emergence as a global economic power. It transformed the social fabric, economic landscape, and technological capabilities of the nation. The lessons learned from this era, including the importance of innovation, infrastructure, labor rights, and environmental stewardship, continue to resonate in contemporary discussions about economic development and industrialization.\nWhile the Industrial Revolution brought about significant progress and prosperity, it also posed challenges that required societal adaptation and regulatory measures. Understanding this complex period is crucial for appreciating the dynamics of modern industrial economies and addressing the ongoing impacts of industrialization in the 21st century."
  },
  {
    "objectID": "blog/06-27-2024-Industrial-Revolution/index.html#support",
    "href": "blog/06-27-2024-Industrial-Revolution/index.html#support",
    "title": "The American Industrial Revolution",
    "section": "",
    "text": "You’ve reached the end of my blog post! 🎉🎉🎉\nIf you learned something and would like to support me, feel free to buy me a coffee :)"
  },
  {
    "objectID": "projects/alzheimers-detection - Copy/index.html",
    "href": "projects/alzheimers-detection - Copy/index.html",
    "title": "Alzheimer’s Detection",
    "section": "",
    "text": "Source Code\n\nBackground\nAlzheimer’s is a type of dementia that affects memory, thinking, and behavior. It is caused by increasing age, and primarily affects people above the age of 65. As a person develops Alzheimer’s, it progressively becomes worse where the individual can lose the ability to carry a conversation or even take care of themselves. After diagnosis, a person can expect to live on average between 4 to 8 years, but on better cases up to 20 years. Luckily there is medication to help slow the worsening of Alzheimer’s, but nothing to completely prevent it from happening.\nThe data used for the detection of Alzheimer’s through handwriting comes from the DARWIN (Diagnosis AlzheimeR WIth haNdwriting) dataset. This dataset is made up of 174 individual’s handwriting where roughly half are Alzheimer’s patients (P), and healthy people (H). The handwriting was taken through tasks the individuals were asked to do, and then variables like time in air were measured. In doing so, the creators of the DARWIN dataset provided us the materials we need to use machine learning techniques to detect the early stages of Alzheimer’s through handwriting. Some of the tasks recorded were connecting points through lines and copying phrases that were written in front of them, all of which test different parts of the brain.\nUsing handwriting data, I will use a random forest classifier to predict whether an individual has Alzheimer’s or not. The goal is for future handwriting data to be inserted and accurately predict the correct diagnosis, saving the individual time to get treatment to slow down the process.\nAlzheimers detection dataset obtained from https://www.kaggle.com/datasets/taeefnajib/handwriting-data-to-detect-alzheimers-disease.\n\n# Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Loading data\nalz = pd.read_csv(\"alzheimers.csv\")\n\n\n\nExploratory Data Analysis\n\n# First 5 rows of data\nalz.head(5)\n\n\n\n\n\n\n\n\nID\nair_time1\ndisp_index1\ngmrt_in_air1\ngmrt_on_paper1\nmax_x_extension1\nmax_y_extension1\nmean_acc_in_air1\nmean_acc_on_paper1\nmean_gmrt1\n...\nmean_jerk_in_air25\nmean_jerk_on_paper25\nmean_speed_in_air25\nmean_speed_on_paper25\nnum_of_pendown25\npaper_time25\npressure_mean25\npressure_var25\ntotal_time25\nclass\n\n\n\n\n0\nid_1\n5160\n0.000013\n120.804174\n86.853334\n957\n6601\n0.361800\n0.217459\n103.828754\n...\n0.141434\n0.024471\n5.596487\n3.184589\n71\n40120\n1749.278166\n296102.7676\n144605\nP\n\n\n1\nid_2\n51980\n0.000016\n115.318238\n83.448681\n1694\n6998\n0.272513\n0.144880\n99.383459\n...\n0.049663\n0.018368\n1.665973\n0.950249\n129\n126700\n1504.768272\n278744.2850\n298640\nP\n\n\n2\nid_3\n2600\n0.000010\n229.933997\n172.761858\n2333\n5802\n0.387020\n0.181342\n201.347928\n...\n0.178194\n0.017174\n4.000781\n2.392521\n74\n45480\n1431.443492\n144411.7055\n79025\nP\n\n\n3\nid_4\n2130\n0.000010\n369.403342\n183.193104\n1756\n8159\n0.556879\n0.164502\n276.298223\n...\n0.113905\n0.019860\n4.206746\n1.613522\n123\n67945\n1465.843329\n230184.7154\n181220\nP\n\n\n4\nid_5\n2310\n0.000007\n257.997131\n111.275889\n987\n4732\n0.266077\n0.145104\n184.636510\n...\n0.121782\n0.020872\n3.319036\n1.680629\n92\n37285\n1841.702561\n158290.0255\n72575\nP\n\n\n\n\n5 rows × 452 columns\n\n\n\n\n# Shape of data\nalz.shape\n\n(174, 452)\n\n\n\n# Data information\nalz.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 174 entries, 0 to 173\nColumns: 452 entries, ID to class\ndtypes: float64(300), int64(150), object(2)\nmemory usage: 614.6+ KB\n\n\n\n# Checking for object column names\nalz.select_dtypes(include = \"object\").columns.tolist()\n\n['ID', 'class']\n\n\n\n# Checking for missing values\nalz.isna().sum() # No NA values\n\nID                 0\nair_time1          0\ndisp_index1        0\ngmrt_in_air1       0\ngmrt_on_paper1     0\n                  ..\npaper_time25       0\npressure_mean25    0\npressure_var25     0\ntotal_time25       0\nclass              0\nLength: 452, dtype: int64\n\n\n\n\nFeature Engineering\n\n# Removing ID column\nalz = alz.drop(\"ID\", axis = 1)\nalz.head(5)\n\n\n\n\n\n\n\n\nair_time1\ndisp_index1\ngmrt_in_air1\ngmrt_on_paper1\nmax_x_extension1\nmax_y_extension1\nmean_acc_in_air1\nmean_acc_on_paper1\nmean_gmrt1\nmean_jerk_in_air1\n...\nmean_jerk_in_air25\nmean_jerk_on_paper25\nmean_speed_in_air25\nmean_speed_on_paper25\nnum_of_pendown25\npaper_time25\npressure_mean25\npressure_var25\ntotal_time25\nclass\n\n\n\n\n0\n5160\n0.000013\n120.804174\n86.853334\n957\n6601\n0.361800\n0.217459\n103.828754\n0.051836\n...\n0.141434\n0.024471\n5.596487\n3.184589\n71\n40120\n1749.278166\n296102.7676\n144605\nP\n\n\n1\n51980\n0.000016\n115.318238\n83.448681\n1694\n6998\n0.272513\n0.144880\n99.383459\n0.039827\n...\n0.049663\n0.018368\n1.665973\n0.950249\n129\n126700\n1504.768272\n278744.2850\n298640\nP\n\n\n2\n2600\n0.000010\n229.933997\n172.761858\n2333\n5802\n0.387020\n0.181342\n201.347928\n0.064220\n...\n0.178194\n0.017174\n4.000781\n2.392521\n74\n45480\n1431.443492\n144411.7055\n79025\nP\n\n\n3\n2130\n0.000010\n369.403342\n183.193104\n1756\n8159\n0.556879\n0.164502\n276.298223\n0.090408\n...\n0.113905\n0.019860\n4.206746\n1.613522\n123\n67945\n1465.843329\n230184.7154\n181220\nP\n\n\n4\n2310\n0.000007\n257.997131\n111.275889\n987\n4732\n0.266077\n0.145104\n184.636510\n0.037528\n...\n0.121782\n0.020872\n3.319036\n1.680629\n92\n37285\n1841.702561\n158290.0255\n72575\nP\n\n\n\n\n5 rows × 451 columns\n\n\n\n\n# Converting class to numeric\nalz[\"class\"] = alz[\"class\"].replace({'P': 1, 'H': 0})\nalz[\"class\"]\n\nC:\\Users\\cor3y\\AppData\\Local\\Temp\\ipykernel_11244\\2961317950.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  alz[\"class\"] = alz[\"class\"].replace({'P': 1, 'H': 0})\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n169    0\n170    0\n171    0\n172    0\n173    0\nName: class, Length: 174, dtype: int64\n\n\n\n\nModel Training\n\nfrom sklearn.model_selection import train_test_split\n\n# Separating features from target\nX = alz.drop(columns=[\"class\"])\ny = alz[\"class\"]\n\n# Training data with a 70/30 split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state = 42)\n\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import plot_tree\n\n# Creating random forest pipeline with scaled data\npipe = Pipeline([\n    ('scaler', StandardScaler()),\n    ('classifier', RandomForestClassifier(random_state = 42, max_samples = 0.6, min_samples_leaf = 2))\n])\n\n# Fitting pipeline\npipe.fit(X_train, y_train)\n\n# Predicting target values\ny_pred = pipe.predict(X_test)\n\n\n# Plotting first tree in the random forest\ntree_viz = pipe.named_steps['classifier'].estimators_[0]\n\nfig, ax = plt.subplots(figsize = (15, 10))\n\nplot_tree(tree_viz, feature_names = alz.columns.tolist(), class_names = [\"Patient\", \"Healthy\"], filled = True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plotting fiftieth tree in the random forest\ntree_viz = pipe.named_steps['classifier'].estimators_[49]\n\nfig, ax = plt.subplots(figsize = (15, 10))\n\nplot_tree(tree_viz, feature_names = alz.columns.tolist(), class_names = [\"Patient\", \"Healthy\"], filled = True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nResults\n\nfrom sklearn.metrics import f1_score\n\n# F1 score is high so this random forest model is a good predictor of the target\nf1 = f1_score(y_test, y_pred)\nprint(\"F1 Score:\", f1)\n\nF1 Score: 0.9019607843137255\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n# False positive and true positive rates\nfpr, tpr, _ = roc_curve(y_test, y_pred)\n\n# AUC\nauc = roc_auc_score(y_test, y_pred)\n\n# Plotting ROC curve\nfig, ax = plt.subplots()\n\nax.plot(fpr, tpr, color = 'darkorange', lw = 2, label = 'ROC curve (AUC = {:.2f})'.format(auc))\nax.plot([0, 1], [0, 1], color = 'navy', lw = 2, linestyle = '--')\n\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('Receiver Operating Characteristic (ROC) Curve')\nax.legend(loc = \"lower right\")\n\nsns.despine()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix\n\n# Creating confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\n\n# Plotting confusion matrix \nfig, ax = plt.subplots()\n\nsns.heatmap(conf_matrix,\n            annot = True,\n            fmt = 'g',\n            xticklabels = ['Positive', 'Negative'],\n            yticklabels = ['Positive', 'Negative'],\n            cmap = [\"Red\", \"Green\", \"Red\", \"Green\"],\n            cbar = False,\n            annot_kws = {\"size\": 20},\n            ax = ax)\n\nax.set_title('Confusion Matrix', fontsize = 17)\nax.set_ylabel('Prediction', fontsize = 13)\nax.set_xlabel('Actual', fontsize = 13)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Creating TP/FP/TN/FN\nTP = conf_matrix[1, 1]\nFN = conf_matrix[1, 0]\nTN = conf_matrix[0, 0]\nFP = conf_matrix[0, 1]\n\n# Printing results of predictions\naccuracy = (TP + TN) / (TP + TN + FP + FN)\nprecision = (TP) / (TP + FP)\nsensitivity = TP / (TP + FN)\nspecificity = TN / (TN + FP)\n\nprint(\"Accuracy:\", accuracy)\nprint(\"Precision:\", precision)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Specificity:\", specificity)\n\nAccuracy: 0.9056603773584906\nPrecision: 0.8846153846153846\nSensitivity: 0.92\nSpecificity: 0.8928571428571429"
  },
  {
    "objectID": "projects/nn-from-scratch/index.html",
    "href": "projects/nn-from-scratch/index.html",
    "title": "Neural Network From Scratch",
    "section": "",
    "text": "Source Code"
  },
  {
    "objectID": "projects/nn-from-scratch/index.html#motivation",
    "href": "projects/nn-from-scratch/index.html#motivation",
    "title": "Neural Network From Scratch",
    "section": "Motivation",
    "text": "Motivation\nI have taken many machine learning classes; some where I learned how to code a model, and others where I learned the math behind it. I have worked on a few neural networks before, but never from scratch (without the use of a major NN library). The goal of this project was to obtain a better understanding of how a neural network works from the training, to the math behind the algorithms, to the activation function used."
  },
  {
    "objectID": "projects/nn-from-scratch/index.html#results",
    "href": "projects/nn-from-scratch/index.html#results",
    "title": "Neural Network From Scratch",
    "section": "Results",
    "text": "Results\nYou can review my code to see exactly how it works, but the results from running the model are: \nAfter 20 epochs, the training accuracy was 0.9943, while the test accuracy was 0.9646. This multilayer perceptron resulted in very high training accuracy, with slightly lower test accuracy, meaning it overfit the data slightly, but for something this simple, I’ll take it. Possibly in the future I will modify the code to try to obtain better test accuracy and show results with different metrics."
  },
  {
    "objectID": "projects/la-crime-viz/index.html",
    "href": "projects/la-crime-viz/index.html",
    "title": "Los Angeles Crime Visualized",
    "section": "",
    "text": "Source Code\nThe data in this notebook was taken from https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8 and consists of crime data from Los Angeles, CA (2020-2024).\nAll visualizations, statistics, and insights discussed are based on this dataset and time frame mentioned above only. The objective is to gain a comprehensive understanding of crime in Los Angeles through data exploration and visualization.\nThe source code button above shows when I uploaded the data to GitHub in 2023, but as of 2024 this is the more updated version.\n\nLoading Libraries\n\n# Libraries\nlibrary(tidyverse)\nlibrary(lubridate)\n\n\n\nExploratory Data Analysis\n\n# Importing dataset\ncrime &lt;- read.csv(\"crime.csv\")\nhead(crime)\n\n      DR_NO              Date.Rptd               DATE.OCC TIME.OCC AREA\n1 190326475 03/01/2020 12:00:00 AM 03/01/2020 12:00:00 AM     2130    7\n2 200106753 02/09/2020 12:00:00 AM 02/08/2020 12:00:00 AM     1800    1\n3 200320258 11/11/2020 12:00:00 AM 11/04/2020 12:00:00 AM     1700    3\n4 200907217 05/10/2023 12:00:00 AM 03/10/2020 12:00:00 AM     2037    9\n5 220614831 08/18/2022 12:00:00 AM 08/17/2020 12:00:00 AM     1200    6\n6 231808869 04/04/2023 12:00:00 AM 12/01/2020 12:00:00 AM     2300   18\n  AREA.NAME Rpt.Dist.No Part.1.2 Crm.Cd\n1  Wilshire         784        1    510\n2   Central         182        1    330\n3 Southwest         356        1    480\n4  Van Nuys         964        1    343\n5 Hollywood         666        2    354\n6 Southeast        1826        2    354\n                               Crm.Cd.Desc             Mocodes Vict.Age\n1                         VEHICLE - STOLEN                            0\n2                    BURGLARY FROM VEHICLE      1822 1402 0344       47\n3                            BIKE - STOLEN           0344 1251       19\n4 SHOPLIFTING-GRAND THEFT ($950.01 & OVER)           0325 1501       19\n5                        THEFT OF IDENTITY 1822 1501 0930 2004       28\n6                        THEFT OF IDENTITY 1822 0100 0930 0929       41\n  Vict.Sex Vict.Descent Premis.Cd                                  Premis.Desc\n1        M            O       101                                       STREET\n2        M            O       128            BUS STOP/LAYOVER (ALSO QUERY 124)\n3        X            X       502 MULTI-UNIT DWELLING (APARTMENT, DUPLEX, ETC)\n4        M            O       405                               CLOTHING STORE\n5        M            H       102                                     SIDEWALK\n6        M            H       501                       SINGLE FAMILY DWELLING\n  Weapon.Used.Cd Weapon.Desc Status  Status.Desc Crm.Cd.1 Crm.Cd.2 Crm.Cd.3\n1             NA                 AA Adult Arrest      510      998       NA\n2             NA                 IC  Invest Cont      330      998       NA\n3             NA                 IC  Invest Cont      480       NA       NA\n4             NA                 IC  Invest Cont      343       NA       NA\n5             NA                 IC  Invest Cont      354       NA       NA\n6             NA                 IC  Invest Cont      354       NA       NA\n  Crm.Cd.4                                 LOCATION Cross.Street     LAT\n1       NA  1900 S  LONGWOOD                     AV              34.0375\n2       NA  1000 S  FLOWER                       ST              34.0444\n3       NA  1400 W  37TH                         ST              34.0210\n4       NA 14000    RIVERSIDE                    DR              34.1576\n5       NA                        1900    TRANSIENT              34.0944\n6       NA  9900    COMPTON                      AV              33.9467\n        LON\n1 -118.3506\n2 -118.2628\n3 -118.3002\n4 -118.4387\n5 -118.3277\n6 -118.2463\n\n\n\n# Changing Date.Rptd and DATE.OCC to date format\ncrime$Date.Rptd &lt;- as.Date(crime$Date.Rptd, format = \"%m/%d/%Y\")\ncrime$DATE.OCC &lt;- as.Date(crime$DATE.OCC, format = \"%m/%d/%Y\")\n\n\ntimes_char &lt;- sprintf(\"%04d\", crime$TIME.OCC)\ndatetime &lt;- strptime(times_char, format = \"%H%M\")\ncrime$TIME.OCC &lt;- format(datetime, format = \"%H:%M\")\n\nhead(crime)\n\n      DR_NO  Date.Rptd   DATE.OCC TIME.OCC AREA AREA.NAME Rpt.Dist.No Part.1.2\n1 190326475 2020-03-01 2020-03-01    21:30    7  Wilshire         784        1\n2 200106753 2020-02-09 2020-02-08    18:00    1   Central         182        1\n3 200320258 2020-11-11 2020-11-04    17:00    3 Southwest         356        1\n4 200907217 2023-05-10 2020-03-10    20:37    9  Van Nuys         964        1\n5 220614831 2022-08-18 2020-08-17    12:00    6 Hollywood         666        2\n6 231808869 2023-04-04 2020-12-01    23:00   18 Southeast        1826        2\n  Crm.Cd                              Crm.Cd.Desc             Mocodes Vict.Age\n1    510                         VEHICLE - STOLEN                            0\n2    330                    BURGLARY FROM VEHICLE      1822 1402 0344       47\n3    480                            BIKE - STOLEN           0344 1251       19\n4    343 SHOPLIFTING-GRAND THEFT ($950.01 & OVER)           0325 1501       19\n5    354                        THEFT OF IDENTITY 1822 1501 0930 2004       28\n6    354                        THEFT OF IDENTITY 1822 0100 0930 0929       41\n  Vict.Sex Vict.Descent Premis.Cd                                  Premis.Desc\n1        M            O       101                                       STREET\n2        M            O       128            BUS STOP/LAYOVER (ALSO QUERY 124)\n3        X            X       502 MULTI-UNIT DWELLING (APARTMENT, DUPLEX, ETC)\n4        M            O       405                               CLOTHING STORE\n5        M            H       102                                     SIDEWALK\n6        M            H       501                       SINGLE FAMILY DWELLING\n  Weapon.Used.Cd Weapon.Desc Status  Status.Desc Crm.Cd.1 Crm.Cd.2 Crm.Cd.3\n1             NA                 AA Adult Arrest      510      998       NA\n2             NA                 IC  Invest Cont      330      998       NA\n3             NA                 IC  Invest Cont      480       NA       NA\n4             NA                 IC  Invest Cont      343       NA       NA\n5             NA                 IC  Invest Cont      354       NA       NA\n6             NA                 IC  Invest Cont      354       NA       NA\n  Crm.Cd.4                                 LOCATION Cross.Street     LAT\n1       NA  1900 S  LONGWOOD                     AV              34.0375\n2       NA  1000 S  FLOWER                       ST              34.0444\n3       NA  1400 W  37TH                         ST              34.0210\n4       NA 14000    RIVERSIDE                    DR              34.1576\n5       NA                        1900    TRANSIENT              34.0944\n6       NA  9900    COMPTON                      AV              33.9467\n        LON\n1 -118.3506\n2 -118.2628\n3 -118.3002\n4 -118.4387\n5 -118.3277\n6 -118.2463\n\n\n\n# Finding dimension of crime dataset\ndim(crime)\n\n[1] 959241     28\n\n\nThis dataset has 959,241 rows and 28 columns.\n\n# Applying NA function to each column\ncrime_is_na &lt;- sapply(crime, function(x) sum(is.na(x)))\ncrime_is_na[which(crime_is_na &gt; 0)]\n\n     Premis.Cd Weapon.Used.Cd       Crm.Cd.1       Crm.Cd.2       Crm.Cd.3 \n            13         634058             11         890716         956944 \n      Crm.Cd.4 \n        959177 \n\n\nPremis.Cd has some missing values. Weapon.Used.Cd has so many missing values because not all recorded crimes involve a weapon, and Crm.Cd.2-4 have many missing values because they will only have a value if more than 1 crime was committed in the same instance. I will not be removing any rows that contain missing values because if I do, there will not be nearly as much data. As a result, I will account for missing values during statistical tests.\n\nWhat are the different types of crimes recorded?\n\n# Finding length of crime list\nlength(table(crime$Crm.Cd.Desc))\n\n[1] 139\n\n\nThere are 139 different types of crimes that have been committed.\n\n# Sorting the number of occurrences for each crime\nsorted_crimes &lt;- table(crime$Crm.Cd.Desc) %&gt;% sort(decreasing = TRUE)\n\nThere are too many crimes to print out the name of each one, but if you’d like you can download this rmd and print the sorted_crimes variable.\n\nhead(sorted_crimes)\n\n\n                                       VEHICLE - STOLEN \n                                                 105537 \n                               BATTERY - SIMPLE ASSAULT \n                                                  74596 \n                                  BURGLARY FROM VEHICLE \n                                                  59476 \n                                      THEFT OF IDENTITY \n                                                  59327 \nVANDALISM - FELONY ($400 & OVER, ALL CHURCH VANDALISMS) \n                                                  58180 \n                                               BURGLARY \n                                                  57597 \n\n\nHere are the 5 highest occurring crimes.\n\ntail(sorted_crimes)\n\n\n      INCEST (SEXUAL ACTS BETWEEN BLOOD RELATIVES) \n                                                 6 \n                     THEFT, COIN MACHINE - ATTEMPT \n                                                 6 \nFIREARMS EMERGENCY PROTECTIVE ORDER (FIREARMS EPO) \n                                                 5 \n                DISHONEST EMPLOYEE ATTEMPTED THEFT \n                                                 4 \n          FIREARMS RESTRAINING ORDER (FIREARMS RO) \n                                                 4 \n                                    TRAIN WRECKING \n                                                 1 \n\n\nHere are the 5 lowest occurring crimes.\n\n\nHow many of the crimes were only attempted?\n\n# Finding number of crimes with \"ATTEMPT\" in title\nsum(grepl(\"ATTEMPT\", crime$Crm.Cd.Desc, ignore.case = TRUE))\n\n[1] 15716\n\n\nOut of all the crimes committed, 15,716 of them were attempts.\n\n\nHow many crimes involved stolen property?\n\n# Finding number of crimes with \"STOLEN\" in title\nsum(grepl(\"STOLEN\", crime$Crm.Cd.Desc, ignore.case = TRUE))\n\n[1] 122940\n\n\nOut of all the crimes committed, 122,940 of them involved stolen property.\n\n\nWhich days had the most amount of crime? The least?\n\n# Sorting dates of crimes based on how many occurred that day\ndate_occ_counts &lt;- crime %&gt;% count(DATE.OCC) %&gt;% arrange(desc(n))\n\n\nhead(date_occ_counts)\n\n    DATE.OCC    n\n1 2020-01-01 1149\n2 2023-01-01 1137\n3 2022-12-02 1132\n4 2023-02-01 1090\n5 2022-10-01 1077\n6 2022-12-01 1056\n\n\nThe date with the most amount of crime is 01/01/2020 with 1149 instances.\n\ntail(date_occ_counts)\n\n       DATE.OCC   n\n1646 2024-07-03 189\n1647 2024-07-05 172\n1648 2024-07-04 171\n1649 2024-07-06 170\n1650 2024-07-07 121\n1651 2024-07-08  48\n\n\nThe date with the least amount of crime is 07/08/2024 with 48 instances.\n\n\n\nGraphing the Data and Conducting Statistical Analysis\n\nWhich areas had the most amount of crime? The least?\n\narea_name_counts &lt;- crime %&gt;% count(AREA.NAME) %&gt;% arrange(desc(n))\n\n\nggplot(head(area_name_counts, 7), aes(x = fct_inorder(AREA.NAME), y = n, fill = fct_inorder(AREA.NAME))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = n), vjust = -0.5) +\n  labs(x = \"Area\", y = \"# of Crimes Comitted\", title = \"Areas with the Highest Number of Crimes Comitted\") +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\") +\n  scale_y_continuous(limits = c(0, 60000))\n\n\n\n\n\n\n\n\nThe area with the highest amount of crime is77th Street with 59,889 reported cases.\n\nggplot(tail(area_name_counts, 7), aes(x = fct_inorder(AREA.NAME), y = n, fill = fct_inorder(AREA.NAME))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = n), vjust = -0.5) +\n  labs(x = \"Area\", y = \"# of Crimes Comitted\", title = \"Areas with the Lowest Number of Crimes Comitted\") +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\") +\n  scale_y_continuous(limits = c(0, 40000))\n\n\n\n\n\n\n\n\nThe area with the lowest amount of crime is Foothill with 31,928 reported cases.\n\n\nWhat premise had a crime occur the most? The least?\n\npremise_counts &lt;- crime %&gt;% count(Premis.Desc) %&gt;% arrange(desc(n))\n\n\nggplot(head(premise_counts, 7), aes(x = fct_inorder(Premis.Desc), y = n, fill = fct_inorder(Premis.Desc))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  labs(x = \"Premise\", y = \"# of Crimes\", title = \"Premise Where Crime Occured the Most\") +\n  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_blank(), axis.ticks.x = element_blank()) +\n  scale_y_continuous(limits = c(0, 220000)) +\n  guides(fill = guide_legend(title = \"Legend\"))\n\n\n\n\n\n\n\n\nThe premise where the most crime occurs are single family dwellings.\n\nggplot(tail(premise_counts, 7), aes(x = fct_inorder(Premis.Desc), y = n, fill = fct_inorder(Premis.Desc))) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = n), vjust = -0.5, size = 3) +\n  labs(x = \"Premise\", y = \"# of Crimes\", title = \"Premise Where Crime Occured the Least\") +\n  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_blank(), axis.ticks.x = element_blank()) +\n  scale_y_continuous(limits = c(0, 2.2)) +\n  guides(fill = guide_legend(title = \"Legend\"))\n\n\n\n\n\n\n\n\nThe premise where the least crime occurs are trams/streetcars.\n\n\nShow the number of crimes commited per age of victim\n\ncrime_per_age &lt;- table(crime$Vict.Age)[5:103] %&gt;% as.data.frame()\n\n\nggplot(crime_per_age, aes(x = Var1, y = Freq, fill = Var1)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Age of Victim\", y = \"# of Crimes\", title = \"Number of Crimes per Victim Age\") +\n  theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 5), legend.position = \"none\")\n\n\n\n\n\n\n\n\nWe can see most victims are aged 19-50, where more children and elderly people are less victimized.\n\n# Creating age groups\ncrime_per_age$group &lt;- cut(as.numeric(crime_per_age$Var1), breaks = c(0, 12, 17, 24, 39, 59, 120), labels = c(\"1-12\", \"13-17\", \"18-24\", \"25-39\", \"40-59\", \"60-120\"))\n\n# Finding age group proportions\ncrime_per_age &lt;- crime_per_age %&gt;% group_by(group) %&gt;% mutate(Proportion = Freq / sum(Freq))\n\n\nggplot(crime_per_age, aes(x = group, y = Proportion, fill = group)) +\n  geom_violin() +\n  stat_summary(fun = median, geom=\"point\", size=2, color=\"red\") +\n  labs(x = \"Age Group\", y = \"Proportion\", title = \"Distribution of Crime Proportions by Age Group\") + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFrom the violin chart, we can see the age group with the highest number of victims is 13-17. The red dots, representing the median, show the 13-17 age group takes up about 20% of the data. The lowest proportion group is the 60-120 year old group, likely due to the older individuals having less chance to become victim of a crime and it is skewed.\n\nggplot(crime[crime$Vict.Age &gt; 0,], aes(x = Vict.Age)) +\n  geom_boxplot(fill = \"green\") +\n  labs(x = \"Age\", title = \"Victim Age\") +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nMedian age of victim seems to be around 37 years old with an IQR of approximately 28-50 years old.\n\n\nHow many crimes occured with a weapon or force?\n\n# Finding number of crimes committed with a weapon\ncrime_weapon_na_count &lt;- sum(is.na(crime$Weapon.Used.Cd))\ncrime_weapon_count &lt;- 788767 - crime_weapon_na_count\n\nweapons &lt;- data.frame(\n  Weapon = c(\"Yes\", \"No\"),\n  cc = c(crime_weapon_count, crime_weapon_na_count)\n)\n\n\nggplot(weapons, aes(x = Weapon, y = cc, fill = Weapon)) +\n  geom_bar(stat = \"identity\") +\n  labs(x = \"Weapon\", y = \"Counts\", title = \"How Many Crimes Were Committed With A Weapon?\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n\n\n\n\n\nWe can see there’s less crimes committed with a weapon than without one, more than a third less.\n\n\nIs there a difference in the proportion of women who are victim of a crime to men?\n\nmale_victims &lt;- subset(crime, Vict.Sex == \"M\")\nfemale_victims &lt;- subset(crime, Vict.Sex == \"F\")\n\nprop.test(\n  x = c(nrow(female_victims), nrow(male_victims)),\n  n = c(788767,788767)\n)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(nrow(female_victims), nrow(male_victims)) out of c(788767, 788767)\nX-squared = 4590.5, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.05538834 -0.05227591\nsample estimates:\n   prop 1    prop 2 \n0.4407461 0.4945782 \n\n\nWith a p-value close to 0 and an alpha level of 0.05, we can reject the null hypothesis and state that the true proportion of victims of a crime who are woman or who are men are different.\n\n\nAre woman victims of certain crimes more often than men?\n\ngender_crime &lt;- subset(crime, Vict.Sex == \"M\" | Vict.Sex == \"F\")\n\n# Using Chi-Square test of Independence\nchisq.test(table(gender_crime$Vict.Sex, gender_crime$Crm.Cd.Desc))\n\n\n    Pearson's Chi-squared test\n\ndata:  table(gender_crime$Vict.Sex, gender_crime$Crm.Cd.Desc)\nX-squared = 75535, df = 136, p-value &lt; 2.2e-16\n\n\nWith a p-value close to 0 and an alpha level of 0.05, we can reject the null hypothesis, indicating a statistically significant association between gender and crime type on an individual.\n\n\nWhat premise had the most vandalisms? The least?\n\n# Counting number of vandalism reports at each premise\npremise_vandalisms &lt;- crime %&gt;% filter(grepl(\"VANDALISM\", Crm.Cd.Desc, ignore.case = TRUE)) %&gt;% group_by(Premis.Desc) %&gt;% summarize(Count = n()) %&gt;% arrange(desc(Count))\n\n\nggplot(head(premise_vandalisms, 10), aes(x = fct_inorder(Premis.Desc), y = Count)) +\n  geom_segment(aes(x = fct_inorder(Premis.Desc), xend = Premis.Desc, y = 0, yend = Count)) +\n  geom_point(color = \"orange\", size = 4) +\n  coord_flip() +\n  labs(x = \"Premise\", y = \"Count\", title = \"Most # of Vandalisms at Premises in LA\") +\n  theme_light() +\n  theme(axis.text.y = element_text(size = 6), plot.title = element_text(hjust = 0.5), \n        panel.grid.major.x = element_blank(), panel.border = element_blank(), \n        plot.margin = margin(c(10,20,10,10), unit = \"pt\")\n        )\n\n\n\n\n\n\n\n\nThe premise with the highest number of reported vandalism are vehicles (passenger/trucks).\n\nggplot(tail(premise_vandalisms, 10), aes(x = fct_inorder(Premis.Desc), y = Count)) +\n  geom_segment(aes(x = fct_inorder(Premis.Desc), xend = Premis.Desc, y = 0, yend = Count)) +\n  geom_point(color = \"orange\", size = 4) +\n  coord_flip() +\n  labs(x = \"Premise\", y = \"Count\", title = \"Most # of Vandalisms at Premises in LA\") +\n  theme_light() +\n  theme(axis.text.y = element_text(size = 6), plot.title = element_text(hjust = 0.5), \n        panel.grid.major.x = element_blank(), panel.border = element_blank(), \n        plot.margin = margin(c(10,20,10,10), unit = \"pt\")\n        )\n\n\n\n\n\n\n\n\nSome MTA lines have the lowest number of vandalisms, with only 1 recorded, as well as a skating rink and tool shed having only 1 instance. In fact, there’s many more which only have 1 reported vandalism.\n\n\nWhat percent of crimes are still being investigated?\n\nsum(grepl(\"Cont\", crime$Status.Desc, ignore.case = TRUE)) / nrow(crime)\n\n[1] 0.7979997\n\n\n79.80% of crimes are still being investigated.\n\n\nAre certain ethnic groups victims of crimes more often than others?\n\ndescent_table &lt;- table(crime$Vict.Descent)[3:21]\nprob = rep(1/length(descent_table), length(descent_table))\n\nchisq.test(descent_table, p = prob)\n\n\n    Chi-squared test for given probabilities\n\ndata:  descent_table\nX-squared = 2713681, df = 18, p-value &lt; 2.2e-16\n\n\nWith a p-value is close to 0, and alpha = 0.5, we can reject the null and state that ethnic groups are not equally as likely to be targeted for a crime.\n\n\nAre there any seasons where crime is more frequent? Less frequent?\n\n# Grouping the date and number of crimes\ndaily_crime &lt;- crime %&gt;% group_by(DATE.OCC) %&gt;% summarize(Count = n())\n\n\n  ggplot(daily_crime, aes(x = DATE.OCC, y = Count)) +\n    geom_line() +\n    facet_wrap(~ year(DATE.OCC), ncol = 3, nrow = 2, scales = \"free\") +\n    labs(x = \"Time\", title = \"Crime over Time\") +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    scale_x_date(date_breaks = \"1 month\", date_labels = \"%b\")\n\n\n\n\n\n\n\n\nIt seems like the beginning of each month there’s a spike in crime where the biggest spike is in January.\n\n\nIs there a certain month that has more crime yearly than others?\nIf we take a look at crime from the graph above, we can see that there’s not really a specific month that generally has more or less crime than others.\n\n\nHas crime increased over time in any area?\n\n# Grouping the top 12 areas by date and number of crimes\nselected &lt;- area_name_counts[1:12,1]\nfiltered_crime &lt;- crime %&gt;% filter(AREA.NAME %in% selected)\ncrime_time_area &lt;- filtered_crime %&gt;% group_by(DATE.OCC, AREA.NAME) %&gt;% summarize(Count = n())\n\n\n  ggplot(crime_time_area, aes(x = DATE.OCC, y = Count)) +\n    geom_line() +\n    facet_wrap(~ AREA.NAME, scales = \"free\") +\n    labs(x = \"Time\", title = \"Crime over Time\") +\n    theme(plot.title = element_text(hjust = 0.5)) +\n    stat_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nNo area seems to have a significant increase in crime over time, but some areas like Central, Newton, Olympic, Pacific, and West LA do have some increase in crime. The areas all seem to have some major spikes in crime on certain days, but nothing that overlaps with each other.\n\n\nAre there certain times of the day where crime occurs more? What about day vs night?\n\n# Convert military time to hours and minutes\nhours &lt;- as.numeric(substr(crime$TIME.OCC, 1, 2))\nminutes &lt;- as.numeric(substr(crime$TIME.OCC, 4, 5))\n\n# Calculate the intervals\ninterval &lt;- sprintf(\"%02d:%02d\", floor((hours * 60 + minutes) %/% 30 / 2), ((hours * 60 + minutes) %/% 30 %% 2) * 30)\ninterval_df &lt;- as.data.frame(table(interval))\n\nnumber_of_bar &lt;- nrow(interval_df)\nangle &lt;- 90 - 360 * (as.numeric(interval_df$interval) - 0.5) / number_of_bar\n\ninterval_df$angle&lt;-ifelse(angle &lt; -90, angle + 180, angle)\n\n\n# Plot of crime frequency per 30 minutes\nggplot(interval_df, aes(x = as.factor(interval), y = Freq, fill = Freq)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_text(aes(x = interval, y = Freq, label = interval, hjust = 0), \n            color = \"black\", fontface = \"bold\", alpha = 0.6, size = 2, angle = angle, \n            inherit.aes = FALSE) +\n  scale_fill_gradient(low = \"green\", high = \"red\") +\n  theme_minimal() +\n  theme(\n    axis.text = element_blank(),\n    panel.grid = element_blank(),\n    axis.title = element_blank(),\n    plot.margin = unit(rep(-0.5, 4), \"cm\")\n  ) +\n  coord_polar(start = 0) +\n  scale_y_continuous(limits = c(-30000, 40000))\n\n\n\n\n\n\n\n\nWe can see that as the time of day increases, the frequency of crime increases, but there is a major spike at noon (12:00).\nIf you’ve made it to the end thank you! I am trying to upload projects using Quarto and I’m still getting used to everything, so I hope to make this prettier eventually!"
  },
  {
    "objectID": "blog/AI-Art-Ethics/index.html",
    "href": "blog/AI-Art-Ethics/index.html",
    "title": "Is AI Art Bad?",
    "section": "",
    "text": "Introduction\nArt made by artificial intelligence has been booming lately because of image generation models like OpenAI’s Dall-E 3. These models have been used to create images that a lot of people call art, but is it really considered art at all? What are the ethics behind it? Does prompt engineering really mean a computer can generate art like a human can?\n\n\n\nSupport \nYou’ve reached the end of my blog post! 🎉🎉🎉\nIf you learned something and would like to support me, feel free to buy me a coffee :)\n\n\n\nComments"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html",
    "href": "blog/Text-Preprocessing-NLP/index.html",
    "title": "Text Preprocessing for NLP",
    "section": "",
    "text": "Natural language processing (NLP) is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and human language. It is used for things like sentiment analysis, product reviews, text summary, text generation, language translation, and countless more. Essentially, if you want to solve an issue that involves natural language, it’s an NLP problem.\nThe early days of NLP involved a bunch of scientists creating if-else statements to support a large set of rules, so it was very limited on what you can accomplish. Since then, AI has come a long way and we can now use neural networks, like a recurrent neural network (RNN), to work with text data. Luckily for us, frameworks like TensorFlow and PyTorch allow us to easily create these types of models where they can learn from text data and produce whatever we’re looking for. Most people use python for deep learning, so any code mentioned will be python. Some popular libraries for NLP in python are NLTK (Natural Language Tool Kit) which is used for cleaning text data and applying some attributes mentioned later, and TensorFlow for the actual model creation. TensorFlow in this post’s case is used for tokenization, and some other languages get used for smaller parts like spacy, re, spellchecker, and collections.\nWhen you want to create a neural network to run an NLP task, like a Long Short-Term Memory (LSTM) recurrent neural network, you have to clean the text for the computer to be able to find patterns in the first place. When using something like a transformer such as GPT-4 or BERT, most of the text preprocessing is done in the model so it won’t be necessary to clean yourself, but the techniques mentioned bellow assume you want to know how to preprocess text data for non-transformer based models. Text preprocessing usually involves cleaning, tokenizing (breaking sentences into words), and embedding (representing words as vectors where similar words are closer together numerically)."
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#lowercasing",
    "href": "blog/Text-Preprocessing-NLP/index.html#lowercasing",
    "title": "Text Preprocessing for NLP",
    "section": "Lowercasing",
    "text": "Lowercasing\nConvert all text to lowercase to avoid treating something like “Hot” and “hot” as different words.\nPython Example:\n\ndef lowercase(text):\n  return text.lower()"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#urls",
    "href": "blog/Text-Preprocessing-NLP/index.html#urls",
    "title": "Text Preprocessing for NLP",
    "section": "URLs",
    "text": "URLs\nRemove URLs to retain only the text.\nPython Example:\n\nimport re\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#html-tags",
    "href": "blog/Text-Preprocessing-NLP/index.html#html-tags",
    "title": "Text Preprocessing for NLP",
    "section": "HTML Tags",
    "text": "HTML Tags"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#emoticons",
    "href": "blog/Text-Preprocessing-NLP/index.html#emoticons",
    "title": "Text Preprocessing for NLP",
    "section": "Emoticons",
    "text": "Emoticons\nRemove emoticons and emojis, as they do not convey meaning to the computer in the same way words do\nPython Example:\n\nimport re\n\ndef remove_emoticons(text):\n  emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n  return emoji_pattern.sub(r'', text)"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#special-characters-numbers",
    "href": "blog/Text-Preprocessing-NLP/index.html#special-characters-numbers",
    "title": "Text Preprocessing for NLP",
    "section": "Special Characters & Numbers",
    "text": "Special Characters & Numbers\nRemove special characters and numbers, unless they are relevant to your analysis.\nPython Example:\n\nimport re\n\ndef remove_characters(text):\n  return re.sub(r'[^A-Za-z ]+', '', text)"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#stop-words",
    "href": "blog/Text-Preprocessing-NLP/index.html#stop-words",
    "title": "Text Preprocessing for NLP",
    "section": "Stop Words",
    "text": "Stop Words\nRemove stop words, which are common words that do not contribute much to the meaning.\nPython Example:\n\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words(\"english\"))\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stop_words]\n    return ' '.join(filtered_words)"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#spell-correction",
    "href": "blog/Text-Preprocessing-NLP/index.html#spell-correction",
    "title": "Text Preprocessing for NLP",
    "section": "Spell Correction",
    "text": "Spell Correction\nCorrect spelling mistakes to ensure consistency in word representation.\nPython Example:\n\nfrom spellchecker import SpellChecker\n\ndef correct_spellings(text):\n    if not text.strip():\n        return text\n\n    spell = SpellChecker()\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n\n    for word in text.split():\n        if word in misspelled_words:\n            suggested_correction = spell.correction(word)\n            if suggested_correction and \"'\" not in suggested_correction:\n                corrected_text.append(suggested_correction)\n            else:\n                corrected_text.append(word)\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#part-of-speech",
    "href": "blog/Text-Preprocessing-NLP/index.html#part-of-speech",
    "title": "Text Preprocessing for NLP",
    "section": "Part-of-Speech",
    "text": "Part-of-Speech\nTag each word with its part of speech to provide context to the model.\nPython Example:\n\nimport nltk\nnltk.download('averaged_perceptron_tagger')\n\ndef get_wordnet_pos(text):\n    tag = nltk.pos_tag([text])[0][1][0].upper()\n    tag_dict = {\"J\": \"a\",  # Adjective\n                \"N\": \"n\",  # Noun\n                \"V\": \"v\",  # Verb\n                \"R\": \"r\"}  # Adverb\n    return tag_dict.get(tag, \"n\")"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#lemmatizing-vs-stemming",
    "href": "blog/Text-Preprocessing-NLP/index.html#lemmatizing-vs-stemming",
    "title": "Text Preprocessing for NLP",
    "section": "Lemmatizing vs Stemming",
    "text": "Lemmatizing vs Stemming\nOnce you’ve made it this far, should you lemmatize or stem your words? Lemmatization is when you cut the suffix off a word and replace it with the normalized form of the word, while stemming refers to cutting the suffix off completely. A good example of this is if you have the word “running”, the stemmed version would be “runn” while the lemmatized version would be “run”. You can see that by lemmatizing your words, you convert it into it’s root word, while stemming could create almost like a second version of the word, which your model might see as a completely separate word. This is why I highly suggest lemmatizing your words rather than stemming them.\nPython Example:\n\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('punkt')\nnltk.download('wordnet')\n\ndef lemmatize(text):\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in nltk.word_tokenize(text)]\n    return \" \".join(lemmatized_words)"
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#others",
    "href": "blog/Text-Preprocessing-NLP/index.html#others",
    "title": "Text Preprocessing for NLP",
    "section": "Others",
    "text": "Others\nFor this part say that there’s a lot of other things you can do like marking negations, handling domain-specific elements like hashtags, converting all dates to the same format, converting numbers to words, etc. Also say you can either do some of it or a lot less of it depending on their goals and some models either do some of this for you or it might not be necessary, like transformers."
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#other-text-cleaning-options",
    "href": "blog/Text-Preprocessing-NLP/index.html#other-text-cleaning-options",
    "title": "Text Preprocessing for NLP",
    "section": "Other Text Cleaning Options",
    "text": "Other Text Cleaning Options\nThere is more out there that you can do to clean or preprocess your text data. Some examples could be marking negations, handling specific elements like hashtags, converting dates to the same format, converting numbers to words, etc. Some of the cleaning mentioned before you could possibly even not want to do depending on the results you want. It all depends on what your end goal is. If you’re planning on using a transformer in the future, pretty much all this text preprocessing is done fully for you through pre-trained models which would cut a lot of time from this data prepping stage."
  },
  {
    "objectID": "blog/Text-Preprocessing-NLP/index.html#other-options",
    "href": "blog/Text-Preprocessing-NLP/index.html#other-options",
    "title": "Text Preprocessing for NLP",
    "section": "Other Options",
    "text": "Other Options\nThere is more out there that you can do to clean or preprocess your text data. Some examples could be marking negations, handling specific elements like hashtags, converting dates to the same format, converting numbers to words, etc. Some of the cleaning mentioned before you could possibly even avoid, depending on what your end goal is. If you’re planning on using a transformer in the future, pretty much all this text preprocessing is done fully for you through pre-trained models which would cut a lot of time from this stage."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "The Statosphere",
    "section": "",
    "text": "Welcome to my blog, The Statosphere. Currently, this is a place where I can write while I do a lot of self-study on, but not limited to, statistics, machine learning, and various fields of mathematics. Hopefully by writing in a bloggy way, I can help myself learn while teaching you something new or interesting. So put your reading glasses on and get ready to enter The Statosphere.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "The Statosphere",
    "section": "",
    "text": "Welcome to my blog, The Statosphere. Currently, this is a place where I can write while I do a lot of self-study on, but not limited to, statistics, machine learning, and various fields of mathematics. Hopefully by writing in a bloggy way, I can help myself learn while teaching you something new or interesting. So put your reading glasses on and get ready to enter The Statosphere.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\nReading Time\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "**Corey Michaud**",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "notes/statistics.html",
    "href": "notes/statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Hi"
  },
  {
    "objectID": "notes/calculus.html",
    "href": "notes/calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "Calculus\nUNDER CONSTRUCTION: Nothing to see for now!"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nWelcome to my notes section! This part of the website is structured like a book.\nThis is currently un-touched because this is for my future studies where I will take notes on a range of topics."
  },
  {
    "objectID": "notes/references.html",
    "href": "notes/references.html",
    "title": "References",
    "section": "",
    "text": "References\nUNDER CONSTRUCTION: Nothing to see for now!"
  },
  {
    "objectID": "notes/index.html#contents",
    "href": "notes/index.html#contents",
    "title": "Introduction",
    "section": "",
    "text": "Statistics\nCalculus"
  },
  {
    "objectID": "notes/index.html#appendix",
    "href": "notes/index.html#appendix",
    "title": "Introduction",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Statistics",
    "section": "",
    "text": "Hello",
    "crumbs": [
      "Math",
      "Statistics"
    ]
  },
  {
    "objectID": "calculus.html",
    "href": "calculus.html",
    "title": "Calculus",
    "section": "",
    "text": "Hello",
    "crumbs": [
      "Computer Science",
      "Calculus"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "hello",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "notes/R.html",
    "href": "notes/R.html",
    "title": "R",
    "section": "",
    "text": "R\nUNDER CONSTRUCTION: Nothing to see for now!"
  },
  {
    "objectID": "notes/python.html",
    "href": "notes/python.html",
    "title": "Python",
    "section": "",
    "text": "Python\nUNDER CONSTRUCTION: Nothing to see for now!"
  }
]